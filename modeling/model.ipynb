{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea456f-0cfb-4396-ae42-4348ed3867a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc1b92-9c73-4288-b2d6-302f2da51ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/A549.data.csv.gz')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472c03e-30d8-4153-8ab5-65dfb55079bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary to map nucleotides to their one-hot encoded representation\n",
    "nucleotide_dict = {'A': [1, 0, 0, 0],\n",
    "                   'C': [0, 1, 0, 0],\n",
    "                   'G': [0, 0, 1, 0],\n",
    "                   'T': [0, 0, 0, 1],\n",
    "                   'N': [0, 0, 0, 0]}\n",
    "\n",
    "# define a function to one-hot encode a single DNA sequence\n",
    "def one_hot_encode_seq(seq):\n",
    "    return np.array([nucleotide_dict[nuc] for nuc in seq])\n",
    "\n",
    "# function to load sequences and enhancer activity\n",
    "def prepare_input(data_set):\n",
    "    # one-hot encode DNA sequences, apply function\n",
    "    seq_matrix = np.array(data_set['seq'].apply(one_hot_encode_seq).tolist())\n",
    "    print(seq_matrix.shape) # dimensions are (number of sequences, length of sequences, nucleotides)\n",
    "    \n",
    "    # Get output array\n",
    "    Y = data_set['category'].map({'enhancer' : 1, 'negative' : 0, 'neutral' : 0})\n",
    "    \n",
    "    return seq_matrix, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75ba9f-9d4a-4123-94fe-a5fdf282cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = prepare_input(data[data['set'] == \"Train\"])\n",
    "X_valid, Y_valid = prepare_input(data[data['set'] == \"Valid\"])\n",
    "X_test, Y_test = prepare_input(data[data['set'] == \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfed432-7389-4ca0-a26a-656a9beb32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 128,\n",
    "          'epochs': 100,\n",
    "          'early_stop': 20,\n",
    "          'lr': 0.001,\n",
    "          'n_conv_layer': 3,\n",
    "          'num_filters1': 64,\n",
    "          'num_filters2': 16,\n",
    "          'num_filters3': 16,\n",
    "          'kernel_size1': 7,\n",
    "          'kernel_size2': 5,\n",
    "          'kernel_size3': 5,\n",
    "          'n_dense_layer': 2,\n",
    "          'dense_neurons1': 32,\n",
    "          'dense_neurons2': 64,\n",
    "          'dropout_conv': 'no',\n",
    "          'dropout_prob': 0.4,\n",
    "          'pad':'same'}\n",
    "\n",
    "\n",
    "def CNNet(params):\n",
    "\n",
    "    # expects sequences of length 350 with 4 channels, length of DNA sequences\n",
    "    input = kl.Input(shape=(350, 4))\n",
    "\n",
    "    # Body - 4 conv + batch normalization + ReLu activation + max pooling\n",
    "    x = kl.Conv1D(params['num_filters1'], kernel_size=params['kernel_size1'],\n",
    "                  padding=params['pad'],\n",
    "                  name='Conv1D_1')(input)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Activation('relu')(x)\n",
    "    x = kl.MaxPooling1D(2)(x)\n",
    "\n",
    "    for i in range(1, params['n_conv_layer']):\n",
    "        x = kl.Conv1D(params['num_filters'+str(i+1)],\n",
    "                      kernel_size=params['kernel_size'+str(i+1)],\n",
    "                      padding=params['pad'],\n",
    "                      name=str('Conv1D_'+str(i+1)))(x)\n",
    "        x = kl.BatchNormalization()(x)\n",
    "        x = kl.Activation('relu')(x)\n",
    "        x = kl.MaxPooling1D(2)(x)\n",
    "        # add dropout after convolutional layers?\n",
    "        if params['dropout_conv'] == 'yes': x = kl.Dropout(params['dropout_prob'])(x)\n",
    "\n",
    "    # After the convolutional layers, the output is flattened and passed through a series of fully connected/dense layers\n",
    "    # Flattening converts a multi-dimensional input (from the convolutions) into a one-dimensional array (to be connected with the fully connected layers\n",
    "    x = kl.Flatten()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    # Each fully connected layer is followed by batch normalization, ReLU activation, and dropout\n",
    "    for i in range(0, params['n_dense_layer']):\n",
    "        x = kl.Dense(params['dense_neurons'+str(i+1)],\n",
    "                     name=str('Dense_'+str(i+1)))(x)\n",
    "        x = kl.BatchNormalization()(x)\n",
    "        x = kl.Activation('relu')(x)\n",
    "        x = kl.Dropout(params['dropout_prob'])(x)\n",
    "\n",
    "    # Model output\n",
    "    output = kl.Dense(1, activation='sigmoid', name=str('Dense_output'))(x)\n",
    "\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d560803-14b1-4f4e-9c5e-c47f8efe2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with specified parameters\n",
    "model = CNNet(params)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=params['lr']),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_history = model.fit(X_train, Y_train,\n",
    "                          validation_data=(X_valid, Y_valid),\n",
    "                          batch_size=params['batch_size'],\n",
    "                          epochs=params['epochs'],\n",
    "                          callbacks=[EarlyStopping(patience=params['early_stop'], monitor=\"val_loss\", restore_best_weights=True), History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf7972-4970-4357-bf5f-25882b11ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation training metrics\n",
    "\n",
    "plt.plot(train_history.history[str('loss')])\n",
    "plt.plot(train_history.history[str('val_loss')])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Add vertical line at minimum validation loss of combined dev and hk\n",
    "min_val_loss = min(train_history.history['val_loss'])\n",
    "plt.axvline(x=train_history.history['val_loss'].index(min_val_loss), color='red', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e22ae-9425-4189-8eb2-09e77f0bd9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Train loss:\", score[0])\n",
    "print(\"Train accuracy:\", score[1])\n",
    "\n",
    "score = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "print(\"Valid loss:\", score[0])\n",
    "print(\"Valid accuracy:\", score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60e6a7-ea0e-444c-a59b-3973f0db44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2352e34-1fcc-4dae-8a29-62e4824fb5d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "from deeplift.visualization import viz_sequence\n",
    "\n",
    "# select a set of background examples to take an expectation over\n",
    "np.random.seed(seed=123)\n",
    "background = X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]\n",
    "\n",
    "# Prepare DeepExplainer for model output\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough # this is required due to conflict between versions (https://github.com/slundberg/shap/issues/1110)\n",
    "explainer = shap.DeepExplainer(model, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8886b2-90f3-415c-b066-f731bdfbdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute nucleotide contribution scores\n",
    "\n",
    "# get strong enhancers\n",
    "cand_enh = data.loc[(data['set'] == 'Test') &\n",
    "                    (data['pvalue'] < 0.05) &\n",
    "                    (data['log2FC'] > 1)]\n",
    "\n",
    "# sort by strength to have the stongest on top for later plotting\n",
    "cand_enh = cand_enh.sort_values('log2FC', ascending = False)\n",
    "\n",
    "# one-hot encode sequences\n",
    "X_cand_enh, Y_cand_enh = prepare_input(cand_enh)\n",
    "\n",
    "# Predict with model\n",
    "pred_values_enh = model.predict(X_cand_enh).squeeze()\n",
    "\n",
    "# calculate scores, get first element of output\n",
    "shap_values_enh = explainer.shap_values(X_cand_enh)[0]\n",
    "\n",
    "final_contr_scores = shap_values_enh * X_cand_enh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5a13f-3a81-4f24-9f10-79b1ad54fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('Enhancer:', cand_enh.iloc[i]['name'],\n",
    "          ' / Obs act:', '{0:0.2f}'.format(cand_enh.iloc[i]['log2FC']),\n",
    "          ' / Pred prob:','{0:0.2f}'.format(pred_values_enh[i]))\n",
    "    \n",
    "    print('Actual contribution scores')\n",
    "    viz_sequence.plot_weights(final_contr_scores[i], figsize=(20, 2), subticks_frequency=20)\n",
    "\n",
    "    # print('Hypothetical contribution scores')\n",
    "    # viz_sequence.plot_weights(shap_values_enh[i], figsize=(20, 2), subticks_frequency=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4d2a8-6b63-4f40-b52b-2fd472cb24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modisco\n",
    "import modisco.affinitymat.core\n",
    "import modisco.cluster.phenograph.core\n",
    "import modisco.cluster.phenograph.cluster\n",
    "import modisco.cluster.core\n",
    "import modisco.aggregator\n",
    "import modisco.util\n",
    "from modisco.visualization import viz_sequence\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import inspect\n",
    "\n",
    "\n",
    "# check parameters\n",
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }\n",
    "\n",
    "get_default_args(modisco.tfmodisco_workflow.workflow.TfModiscoWorkflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5d3b5-35af-4785-8cd8-20ab0feadf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_scores = OrderedDict()\n",
    "task_to_scores['enhancers'] = final_contr_scores\n",
    "task_to_hyp_scores = OrderedDict()\n",
    "task_to_hyp_scores['enhancers'] = shap_values_enh\n",
    "\n",
    "null_per_pos_scores = modisco.coordproducers.LaplaceNullDist(num_to_samp=5000)\n",
    "\n",
    "\n",
    "# prepare TF-modisco function to run\n",
    "def tfmodisco(X):\n",
    "\n",
    "    one_hot = X\n",
    "\n",
    "    tfmodisco_results = modisco.tfmodisco_workflow.workflow.TfModiscoWorkflow(\n",
    "                        #target_seqlet_fdr controls the stringency of the threshold used.\n",
    "                        # the default value is 0.2\n",
    "                        target_seqlet_fdr=0.2,\n",
    "                        #min_passing_windows_frac and max_passing_windows_frac can be used\n",
    "                        # to manually adjust the percentile cutoffs for importance\n",
    "                        # scores if you feel that the cutoff\n",
    "                        # defined by the null distribution is too stringent or too\n",
    "                        # lenient. The default values are 0.03 and 0.2 respectively.\n",
    "                        #min_passing_windows_frac=0.03,\n",
    "                        #max_passing_windows_frac=0.2\n",
    "                        #The sliding window size and flanks should be adjusted according to the expected length of the core motif and its flanks.\n",
    "                        #If the window size or flank sizes are too long, you risk picking up more noise.\n",
    "                        sliding_window_size=15,\n",
    "                        flank_size=5,\n",
    "                        max_seqlets_per_metacluster=50000,\n",
    "                        seqlets_to_patterns_factory=\n",
    "                            modisco.tfmodisco_workflow\n",
    "                                    .seqlets_to_patterns\n",
    "                                    .TfModiscoSeqletsToPatternsFactory(\n",
    "                                #kmer_len, num_gaps and num_mismatches are used to\n",
    "                                # derive kmer embeddings for coarse-grained affinity\n",
    "                                # matrix calculation. kmer_len=6, num_gaps=1\n",
    "                                # and num_mismatches=0 means\n",
    "                                # that kmer embeddings using 6-mers with 1 gap will be\n",
    "                                # used. The default is to use longer kmers, but this\n",
    "                                # can take a while to run and can lead to\n",
    "                                # out-of-memory errors on some systems.\n",
    "                                # Empirically, 6-mers with 1-gap\n",
    "                                # seem to give good results.\n",
    "                                #During the seqlet clustering, motifs are trimmed to the central trim_to_window_size bp with the highest importance\n",
    "                                trim_to_window_size=15,\n",
    "                                #After the trimming is done, the seqlet is expanded on either side by initial_flank_to_add\n",
    "                                initial_flank_to_add=5,\n",
    "                                final_min_cluster_size=50\n",
    "                        )\n",
    "                   )(\n",
    "                task_names=['enhancers'],\n",
    "                contrib_scores=task_to_scores,\n",
    "                hypothetical_contribs=task_to_hyp_scores,\n",
    "                one_hot=one_hot,\n",
    "                null_per_pos_scores = null_per_pos_scores)\n",
    "\n",
    "    return tfmodisco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939d0f1-d8bc-4483-8e4b-bceb510e67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize motifs\n",
    "\n",
    "def modisco_motif_plots(hdf5_modisco_results):\n",
    "\n",
    "    hdf5_results = h5py.File(hdf5_modisco_results, \"r\")\n",
    "\n",
    "    metacluster_names = [\n",
    "        x.decode(\"utf-8\") for x in\n",
    "        list(hdf5_results[\"metaclustering_results\"][\"all_metacluster_names\"][:])]\n",
    "\n",
    "    all_patterns = []\n",
    "\n",
    "    # sequence background\n",
    "    background = np.mean(X_cand_enh, axis=(0, 1))\n",
    "\n",
    "\n",
    "    for metacluster_name in metacluster_names:\n",
    "        print(metacluster_name)\n",
    "        metacluster_grp = (hdf5_results[\"metacluster_idx_to_submetacluster_results\"]\n",
    "                                       [metacluster_name])\n",
    "        print(\"activity pattern:\",metacluster_grp[\"activity_pattern\"][:])\n",
    "        all_pattern_names = [x.decode(\"utf-8\") for x in\n",
    "                             list(metacluster_grp[\"seqlets_to_patterns_result\"]\n",
    "                                                 [\"patterns\"][\"all_pattern_names\"][:])]\n",
    "        if (len(all_pattern_names)==0):\n",
    "            print(\"No motifs found for this activity pattern\")\n",
    "        for pattern_name in all_pattern_names:\n",
    "            print(metacluster_name, pattern_name)\n",
    "            all_patterns.append((metacluster_name, pattern_name))\n",
    "            pattern = metacluster_grp[\"seqlets_to_patterns_result\"][\"patterns\"][pattern_name]\n",
    "            print(\"total seqlets:\",len(pattern[\"seqlets_and_alnmts\"][\"seqlets\"]))\n",
    "            # print(\"Actual importance scores:\")\n",
    "            # viz_sequence.plot_weights(pattern[str(task + \"_contrib_scores\")][\"fwd\"])\n",
    "            # print(\"Hypothetical scores:\")\n",
    "            # viz_sequence.plot_weights(pattern[str(task + \"_hypothetical_contribs\")][\"fwd\"])\n",
    "            print(\"IC-scaled, fwd and rev:\")\n",
    "            viz_sequence.plot_weights(viz_sequence.ic_scale(np.array(pattern[\"sequence\"][\"fwd\"]),\n",
    "                                                            background=background))\n",
    "            viz_sequence.plot_weights(viz_sequence.ic_scale(np.array(pattern[\"sequence\"][\"rev\"]),\n",
    "                                                            background=background))\n",
    "\n",
    "            #Plot the subclustering too, if available\n",
    "            if (\"subclusters\" in pattern):\n",
    "                print(\"PLOTTING SUBCLUSTERS\")\n",
    "                subclusters = np.array(pattern[\"subclusters\"])\n",
    "                twod_embedding = np.array(pattern[\"twod_embedding\"])\n",
    "                plt.scatter(twod_embedding[:,0], twod_embedding[:,1], c=subclusters, cmap=\"tab20\")\n",
    "                plt.show()\n",
    "                for subcluster_name in list(pattern[\"subcluster_to_subpattern\"][\"subcluster_names\"]):\n",
    "                    subpattern = pattern[\"subcluster_to_subpattern\"][subcluster_name]\n",
    "                    print(subcluster_name.decode(\"utf-8\"), \"size\", len(subpattern[\"seqlets_and_alnmts\"][\"seqlets\"]))\n",
    "                    subcluster = int(subcluster_name.decode(\"utf-8\").split(\"_\")[1])\n",
    "                    plt.scatter(twod_embedding[:,0], twod_embedding[:,1], c=(subclusters==subcluster))\n",
    "                    plt.show()\n",
    "                    # viz_sequence.plot_weights(subpattern[str(task + \"_hypothetical_contribs\")][\"fwd\"])\n",
    "                    # viz_sequence.plot_weights(subpattern[str(task + \"_contrib_scores\")][\"fwd\"])\n",
    "                    viz_sequence.plot_weights(viz_sequence.ic_scale(np.array(subpattern[\"sequence\"][\"fwd\"]),\n",
    "                                                            background=background))\n",
    "\n",
    "    hdf5_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa2239-2a33-4478-9c68-3a0f178c691c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run TF-Modisco\n",
    "tfmodisco_results = tfmodisco(X_cand_enh)\n",
    "\n",
    "# save results\n",
    "hdf5_modisco_results = \"modisco_results.hdf5\"\n",
    "\n",
    "grp = h5py.File(hdf5_modisco_results, \"w\")\n",
    "tfmodisco_results.save_hdf5(grp)\n",
    "grp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1180f-7305-434f-a08d-7ccae0114d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize motifs\n",
    "modisco_motif_plots(hdf5_modisco_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
